<?xml version="1.0" encoding="utf-8"?>
<Workbook xmlns="urn:schemas-microsoft-com:office:spreadsheet" xmlns:x="urn:schemas-microsoft-com:office:excel" xmlns:ss="urn:schemas-microsoft-com:office:spreadsheet" xmlns:html="http://www.w3.org/TR/REC-html40">
  <Styles>
    <Style ss:ID="s1">
      <Font x:Family="Swiss" ss:Bold="1" />
    </Style>
    <Style ss:ID="s2">
      <NumberFormat ss:Format="0%" />
    </Style>
  </Styles>
  <Worksheet ss:Name="ICML2021">
    <Table>
      <Row>
        <Cell ss:Index="1">
          <Data ss:Type="String">ICML2021</Data>
        </Cell>
      </Row>
      <Row ss:Index="3">
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Paper ID</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Paper Title</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Reviewer Name</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Reviewer Email</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Reviewer Number</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Created</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Last Modified</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Q1 ([Summary] Please summarize the main claims/contributions of the paper in your own words (1-2 sentences or paragraphs).
)</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Q2 ([Detailed comments] Describe the strengths and weaknesses of the work, with respect to the following criteria: soundness of the claims (theoretical grounding, empirical evaluation), significance and novelty of the contribution, relation with prior work, clarity of writing, and relevance to the ICML community.)</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Q3 ([Relevance and Significance] (Is the subject matter important? Does the problem it tries to address have broad interests to the ICML audience or has impact in a certain special area? Is the proposed technique important, and will this work influence future development?))</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Q3 ([Relevance and Significance] (Is the subject matter important? Does the problem it tries to address have broad interests to the ICML audience or has impact in a certain special area? Is the proposed technique important, and will this work influence future development?) - Value)</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Q4 ([Novelty] (Is relation to prior work well-explained, does it present a new concept or idea, does it improve the existing methods, or extend the applications of existing practice?) 
)</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Q4 ([Novelty] (Is relation to prior work well-explained, does it present a new concept or idea, does it improve the existing methods, or extend the applications of existing practice?) 
 - Value)</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Q5 ( [Technical quality] (Is the approach technically sound. The claims and conclusions are supported by flawless arguments. Proofs are correct, formulas are correct, there are no hidden assumptions.))</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Q5 ( [Technical quality] (Is the approach technically sound. The claims and conclusions are supported by flawless arguments. Proofs are correct, formulas are correct, there are no hidden assumptions.) - Value)</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Q6 ([Experimental evaluation] (Are the experiments well designed, sufficient, clearly described? The experiments should demonstrate that the method works under the assumed conditions, probe a variety of aspects of the novel methods or ideas, not just the output performance, present comparisons with prior work, test the limits and check the robustness of the novel methods or ideas, and demonstrate their practical relevance.)
)</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Q6 ([Experimental evaluation] (Are the experiments well designed, sufficient, clearly described? The experiments should demonstrate that the method works under the assumed conditions, probe a variety of aspects of the novel methods or ideas, not just the output performance, present comparisons with prior work, test the limits and check the robustness of the novel methods or ideas, and demonstrate their practical relevance.)
 - Value)</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Q7 ([Clarity] (Is the paper well-organized and clearly written, should there be additional explanations or illustrations?)
)</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Q7 ([Clarity] (Is the paper well-organized and clearly written, should there be additional explanations or illustrations?)
 - Value)</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Q8 ([Reproducibility] (are there enough details to reproduce the major results of this work?)
)</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Q9 ([Questions for authors] Please provide questions for authors to address during the author feedback period. (Optional, to help authors focus their response to your review.))</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Q10 (Please provide an "overall score" for this submission.
)</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Q10 (Please provide an "overall score" for this submission.
 - Value)</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Q11 ([Confidence] Please provide your confidence in your assessment of this submission.
)</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Q11 ([Confidence] Please provide your confidence in your assessment of this submission.
 - Value)</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Q12 ( [Confidential comments to SPC, AC, and Program Chairs] (including potential ethical concerns)
)</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Q13 ([Anonymity] Do you believe you know the identities of the paper authors? If yes, please tell us how in the next question.)</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Q14 ([Anonymity details] If you answered Yes above, please tell us when and how you discovered the authors' identities. The answers are not mutually exclusive (nor exhaustive); check all that applies.)</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Q15 ([Handled previously] Have you previously reviewed or area chaired (a version of) this work for another archival venue?

)</Data>
        </Cell>
        <Cell ss:StyleID="s1">
          <Data ss:Type="String">Q16 (Please acknowledge that you have read the author rebuttal. If your opinion has changed, please summarize the main reasons in the Detailed comments sections.
)</Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[The Heavy-Tail Phenomenon in SGD]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Amina Shabbeer]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[aminashabbeer@gmail.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[3/13/2021 6:21:39 PM -08:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[3/13/2021 6:21:39 PM -08:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Summary:
This work aims to characterize generalization properties of SGD in terms of convergence of SGD iterates to a heavy-tailed distribution.
Authors identify that properties previously observed to correlate with good generalization properties: (i) Curvature (Hessian of minima) (ii) Ratio of step-size to batch-size, do so because those choices result in the SGD iterates converging to a heavy-tailed distribution.
They show tail index increases monotonically for increasing curvature and increasing batch-size, and strictly decreasing in step size and dimension d.
Further, authors provide a convergence rate by showing convergence is exponentially fast in the Wasserstein metric.
Proofs are on a strictly convex function. Experimental results estimate the tail-index for various choices of step-size, batch-size for this problem using synthetically generated data drawn from a Gaussian, and demonstrate the proven monotonic relations hold in practice. They show adaptive algorithms like RMSProp and Variance-reduced algorithms don't converge to heavy-tailed distributions, and hypothesize weaker generalization is due to this.

Further experimental results for a fully connected network on MNIST and CIFAR10, and VGG networks on CIFAR10 that estimate the tail-index for various choices of step-size, batch-size demonstrate results may extend to nonconvex setting as well.  ]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Strengths:
Significance: This paper is a strong step in developing our understanding of how choices of parameters during training can improve generalization. The observed correlation of heavy-tailed distribution in SGD iterates is also corroborated by other work e.g., Martin and Mahoney observe heavy-tailed distributions in weights are associated with improved generalization. This paper goes beyond observed correlation and provides theoretical grounding for paramater choices that have been known to work well in practice.

Novelty: To my knowledge, this is the first work that characterizes generalization capabilities in terms of tail-index of the distribution of SGD iterates. This also provides insights into probability of escaping 

Rigor: Proofs are thorough. Lemmas establish h(s) is a convex function. Then for various regimes, e.g., alpha>=1, they show monotonically decreasing alpha for increasing curvature based on the monotonically reducing function h(a, s). 
Convergence rate is established by first providing an upper bound for the iterates x_k governed by probability law v_k. Then showing Wasserstein metric W(v_k, v_inf) is bounded by W(v_0, v_inf) by a factor of h(p)^1/p. 

Weaknesses:
Clarity: Paper would benefit from a notation section. e.g., Function h(s) for recursion of multiplicative noise is defined in the introduction without definition of s (tail-index). Paper should be self-contained - Proof sketches should be provided in the main paper. The Appendix should also be self-contained with definitions prior to introducing proofs.
D3 Proof of Theorem 4 in Appendix, Part I is only in terms of a, which establishes the result for curvature, but not step-size \eta. Can authors clarify?

Relation to existing work: 
(This is a mild weakness, as authors cite several other works where there is alignment.) 
Proofs are on a strongly convex function. Some experimental results are presented on deep networks and indicate adaptive methods do not converge to heavy-tailed distributions. This is connected to some work that claims adaptive methods do not generalize well. In practice though, adaptive methods are used successfully e.g., https://arxiv.org/abs/1912.03194 argue for adaptive methods to avoid heavy-tailed behavior in iterates.
Authors must either state this discrepancy as part of future research as this work is focused on SGD, or make a stronger claim about SGD vs adaptive methods generalization capabilities.
Similarly, all results are on fixed step size, eta  > eta_crit, not learning schedules with reducing step-size often used in practice.
This work does not address the double descent behavior of heavily overparametrized systems often used in practice. Since the proofs are in the streaming setting, paper doesn't quite address the fact that in practice networks are trained to overfitting, yet they generalize. e.g., 
https://arxiv.org/pdf/1906.11300.pdf
 ]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Significant contribution, advances state of the art]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[6]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Several novel and surprising contributions]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[6]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Technically strong, highly general results, advanced techniques]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[6]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Solid, informative evaluation w.r.t all 5 criteria]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[6]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Mostly clear, but improvements needed, as recommended in the detailed comments.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[3]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Yes]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[D3 Proof of Theorem 4 in Appendix, Part I is only in terms of a, which establishes the result for curvature, but not step-size \eta. Can authors clarify?]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept: Good paper]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[5]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[I am quite sure about my evaluation. It's unlikely,  although possible that I missed something that should affect my ratings.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[4]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Yes]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[I discovered the authors unintentionally while searching web for related work during reviewing of this paper.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[The Heavy-Tail Phenomenon in SGD]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Dong Yin]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[dongyin@google.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[5]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[3/10/2021 10:33:29 PM -08:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[4/6/2021 9:57:03 AM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[This paper studies the stationary distribution of SGD. The authors show that for linear regression problems, the stationary distribution has heavy tail. The authors also make connection among the tail index, the ratio of learning rate to batch size, and the curvature of the loss function. The authors show that their theoretical findings also hold empirically in deep learning models.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[I think this paper is good overall. It is well written, relatively easy to follow, and makes solid contributions. The connections among the tail index, the ratio of learning rate to batch size, and the curvature of the loss function are very interesting. As the authors mentioned, all of the three metrics are empirically observed to be related to generalization property of SGD, and the rigorous connections among them is indeed useful. To me the results seem to be new in the field.

I have a few concerns:
1) I understand that to study more complicated models such as deep neural networks, we should begin with simpler problems. However, I wonder how hard it is to extend the results to more general class of problems. It would be interesting to see at least some results that hold for convex loss functions.

2) Follow the previous point, I think even for linear regression problems, it would be interesting to see some more general cases. What if the feature are not i.i.d. Gaussian? What if the covariance matrix is not \sigma^2 times identity? In this case, the curvature of the loss function is different on different directions, and it would be interesting to see how the tail relates to curvatures on different directions. Is it more related to the largest or the smallest singular value? What if the covariance matrix is not full rank? I think for deep learning models, due to overparameterization, the Hessian matrix is usually low rank and thus the curvature is flat along most directions. How does the results address this case, even for linear regression models?

3) One thing that is missing in the paper is that although the connection among the tail index, the ratio of learning rate to batch size, and the curvature of the loss function is interesting, the authors did not provide rigorous proof on how these three metrics are related to the generalization property of the model (the difference between the training and test accuracy).

Minor: 
Authors should pay attention to author-year citation style, i.e., \citet vs \citep: 
Recently, (Jastrz˛ebski et al., 2017) focused on this phenomenon as well and empirically…
There should not be parentheses around Jastrz˛ebski et al., 2017.

Theorem 1:
Condition (i): is M(\Omega) a matrix or a scalar? Does this condition mean that || \Phi_\Omega(x) - M(\Omega)x || <= B(\Omega) needs to uniformly hold for all \Omega?
Condition (ii): extra “)” in log||M(\Omega)||]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Solid contribution to relevant problem]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[5]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Worthy contributions, but not surprising]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[4]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Technically adequate for its area, solid results]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[5]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Sufficient evaluation w.r.t. most criteria]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[5]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Very clear, only minor flaws.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[4]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Yes]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Is it possible to extend the results to more general problem, such as convex loss functions? Or more general linear regression problems?

Is it possible to make rigorous connections between the tail behavior of the stationary distribution and the generalization error of the models?]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept: Good paper]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[5]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[I am knowledgeable and willing to defend my evaluation, but there's a chance I missed something.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[3]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[The Heavy-Tail Phenomenon in SGD]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Nicole Muecke]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[muecke.math@gmail.com]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[6]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[3/13/2021 7:04:29 AM -08:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[4/11/2021 2:14:52 AM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[The authors analyze the connection between generalization and heavy tails in the context of least squares linear regression with SGD. More precisely, 
 it is shown that the law of the iterates converges to a heavy-tail distribution.  If the input data are gaussian, a relation between tail index and stepsize, batchsize and variance is provided.   

Numerical experiments support the theoretical findings. They show that the results obtained for linear regression seem to be extendable to neural 
network learning. ]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[strengths: 
*the paper presents for the first time a mathematical rigorous analysis about the heavy-tail phenomenon for SGD (for linear regression) that has been observed empirically in a number of papers
* the paper is well written easy to follow and pleasant to read

weaknesses: 
*maybe one weakness is the assumption (A3): results hold for a very restricted class of input distributions 

*minor (to improve readability): at various places (e.g. in the abstract or Thm. 2), the authors write 
"SGD iterates converge to a heavy-tailed distribution", " ... stationary distribution x_\infty" 
- I think they mean: "x_\infty has a distribution that satisfies ... " , also, the iterates itselves (or a certain limit of them)
are not distributions ...  ]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Significant contribution, advances state of the art]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[6]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[One idea that surprised me by its originality, solid contributions otherwise]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[5]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Technically adequate for its area, solid results]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[5]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Sufficient evaluation w.r.t. most criteria]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[5]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Very clear, only minor flaws.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[4]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Yes]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Could you comment on which properties of the least-squares allow you to derie your results and what challenges do you expect when using more general losses, e.g. convex, which are used for e.g. classification problems. Thanks!   ]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Accept: Good paper]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[5]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[I am quite sure about my evaluation. It's unlikely,  although possible that I missed something that should affect my ratings.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[4]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
      <Row>
        <Cell>
          <Data ss:Type="Number"><![CDATA[1]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[The Heavy-Tail Phenomenon in SGD]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Sen Na]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[senna@uchicago.edu]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[7]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[3/6/2021 6:59:32 PM -08:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[4/10/2021 10:56:01 AM -07:00]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[The paper studies the heavy-tailed property of the stationary distribution of SGD iterates, with a particular focus on linear model. The authors show the relation of the tail index and the SGD parameters.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Strength: the tail of stationary distribution of SGD iterates is an important problem, which can explain some empirical phenomenon of SGD, for example, why it likes flat minima. 

Weakness: the paper tries to show how the SGD parameters including stepsize and batch size affect the tail index, however the relation is very implicit even for linear model. Only an upper bound tilde{h}(s) is provided, but how does it imply the solution of h(s) = 1.


]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Solid contribution to relevant problem]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[5]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Worthy contributions, but not surprising]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[4]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Technically adequate for its area, solid results]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[5]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Sufficient evaluation w.r.t. most criteria]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[5]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Mostly clear, but improvements needed, as recommended in the detailed comments.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[3]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Yes]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[I think the paper is a borderline paper due to some concerns:

1, some theorems in the paper are not rigorously presented. For example, Theorem 3 and Prop 5, they should under Gaussian assumption. Please include all assumptions in the theorem to make them clear.

2, the technical contribution over (Hodgkinson & Mahoney, 2020) is unclear. The author only mention (Hodgkinson & Mahoney, 2020) study a more general iteration scheme, but what are the technical challenges in this paper. Please emphasize that explicitly.

3, the study of convergence speed with Wasserstein metric seems a bit outside of the target. How is it helpful to understand the tail behavior of iterates.

4, the main theorem 2 has an assumption that is very hard to check. How can one know if alpha s.t. h(alpha) = 1 exists or not. The authors should provide more justification on this.


 ]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[Weak Reject: Borderline, tending to reject]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[3]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[I am knowledgeable and willing to defend my evaluation, but there's a chance I missed something.]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="Number"><![CDATA[3]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[No]]></Data>
        </Cell>
        <Cell>
          <Data ss:Type="String"><![CDATA[]]></Data>
        </Cell>
      </Row>
    </Table>
  </Worksheet>
</Workbook>